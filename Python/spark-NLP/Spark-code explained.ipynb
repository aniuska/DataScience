{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Large-scale text classification using Spark’s MapReduce with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why this code\n",
    "\n",
    "- Big Data Technology\n",
    "- Apply NLP techniques\n",
    "- Use MapReduce programming model\n",
    "- First Python coding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How it works - background\n",
    "\n",
    "- NLP for large-scale text classification ( > 100k books from project Gutenberg)\n",
    "- Feature extraction using TF-IDF (Term frequency * Inverse Doc Frequency ). \n",
    "  TF ~ Document relevance & IDF ~ Term specificity\n",
    "- MapReduce used for processing large volumes of data in parallel\n",
    "- Spark is cluster-computing framework for general-purpose data processing engine (Big Data processing) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How it works - MapReduce explained\n",
    "\n",
    "### Map\n",
    "- Input is (key, value) tuples\n",
    "- A custom function (business logic) is applied to every value in the tuple\n",
    "- Output is a new list of (key,  value) tuples\n",
    "\n",
    "### Reduce\n",
    "- Tuples are sorted by key before to apply Reduce\n",
    "- Input is (key, value) tuple from applying Map\n",
    "- Custom function (usually aggregation or summation) to iterate the values for a given key. \n",
    "- Output is the final list of (key,  value) tuples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How it works - Code explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    config = SparkConf().setMaster(\"local[2]\")\n",
    "    sc = SparkContext(conf=config, appName=\"Coursework - Large-scale text classification\")\n",
    "\n",
    "    #Create corpus\n",
    "    #Read files content and get number of files in the corpus\n",
    "    path = \"/data/extra/gutenberg/text-tiny\"\n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "      for name in files:\n",
    "          newFile = sc.wholeTextFiles( os.path.join(root, name))\n",
    "          if \"textFiles\" in globals():\n",
    "             textFiles = textFiles.union( newFile )\n",
    "          else:\n",
    "              textFiles = newFile\n",
    "\n",
    "    fileNum = textFiles.count()  \n",
    "    \n",
    "    #read our list of commun & stop words\n",
    "    stopwords = sc.textFile(sys.argv[1]).flatMap(lambda x: x.split(',')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    #Text prepocessing\n",
    "    #Create list of tuples (fileId, content) and remove file's header\n",
    "    textIdFiles = textFiles.flatMap(lambda (f,x): [removeHeader(x)] )\n",
    "    \n",
    "    #Create list of tuple (file, list of words). Also remove stop words\n",
    "    fileIdWords = textIdFiles.flatMap(lambda (f,x): [(f, w.strip()) for w in re.split('\\W+',x)])\n",
    "    fileIdWords = fileIdWords.filter(lambda (f,w): w not in stopwords and len(w) > 0)\n",
    "    fileIdWords.cache()\n",
    "\n",
    "    #Remove plural words (simple stemming) & word count\n",
    "    #list of tuples ((fileID,word),1)\n",
    "    wordsCount = fileIdWords.map(lambda (f,x): ((f,remPlural(x)),1)).reduceByKey(add)\n",
    "    #tuples ((fileID,word),count)\n",
    "    #filter infrequent words\n",
    "    wordsCount = wordsCount.filter(lambda x:  x[1] >= 5)\n",
    "\n",
    "    #Adjusting tuples (fileID, (word,count)) \n",
    "    wordsByFiles = wordsCount.map(lambda (fw,c): (fw[0],[(fw[1],c)]))\n",
    "    #Grouping all (word,count) per FileID : tuples (filedID, [(word,count)])\n",
    "    wordsByFiles = wordsByFiles.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculate TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    #Calculate Term frequency(TF) & Inverse Doc Frequency(IDF)\n",
    "    #create list of tuples (fileID,maxfreq,[(word,count)])\n",
    "    wordsByFilesMaxFrecuency = wordsByFiles.map(lambda (f,wc) : ((f, max(wc, key = lambda l: l[1])), wc) )\n",
    "\n",
    "    #Normalise TF - tf/maxfreq (to discount for “longer” documents), creating list of tuples ([fileID,w],[count,TF])\n",
    "    wordsByFilesTF = wordsByFilesMaxFrecuency.flatMap(lambda (fm,wc): [([fm[0], w],[c,c*1.0/fm[1][1]]) for (w,c) in wc])\n",
    "\n",
    "    #Adjust tuples conveniently (word,[(fileID,count,TF )]) \n",
    "    wordsTF = wordsByFilesTF.map(lambda (fw,cft): (fw[1],[(fw[0],cft[0],cft[1])] ) )\n",
    "    wordsTF = wordsTF.reduceByKey(add)\n",
    "    \n",
    "    #calculate TF-IDF and create new tuples (word,[(fileID,tf,idf)])\n",
    "    #Adjust tuples & add idf where idf = log(N/df), df = len(list([]))\n",
    "    wordsTFandIDF = wordsTF.map(lambda (w,fl): (w,[(f,ft,math.log10(fileNum/len(fl))) for (f,c,ft) in fl ]) )\n",
    "\n",
    "    #Calculate TF-IDF, creating list of tuples ([word, fileID],tf-idf)\n",
    "    wordsTFIDF = wordsTFandIDF.flatMap(lambda (w,fl): [([w,f],ft*idf) for (f,ft,idf) in fl] )\n",
    "    #removing tfidf = 0 (irrelevant word) to reduce size\n",
    "    wordsTFIDF = wordsTFIDF.filter(lambda (wf,tfidf): tfidf != 0) \n",
    "\n",
    "    #Reajust tuples (fileID,[word,tf-idf])\n",
    "    fwl_tfidf = wordsTFIDF.map(lambda (fw,tfidf): (fw[1],[(fw[0],tfidf)] ) )\n",
    "    fwl_tfidf = fwl_tfidf.reduceByKey(add)\n",
    "    \n",
    "    hashSize = 10\n",
    "    fw_vec = fwl_tfidf.map(lambda (f,wcl): f_hashVector(f,wcl,hashSize) )"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
